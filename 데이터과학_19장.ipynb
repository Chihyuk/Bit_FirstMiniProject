{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "데이터과학 19장.ipynb",
      "provenance": [],
      "mount_file_id": "1EYsFJv5WKvl2ZN2Ai74rLVPYaNNLb3IN",
      "authorship_tag": "ABX9TyPZuKcdjCn/fMoOxxXtBEr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chihyuk/Genie/blob/master/%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99_19%EC%9E%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝\n",
        "- 한개 이상의 은닉층을 지닌 깊은 신경망\n",
        "- 간단한 신경망을 포함한 다양한 신경망"
      ],
      "metadata": {
        "id": "XfHuIjzFhmN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텐서: n차원의 배열"
      ],
      "metadata": {
        "id": "s8bdMKl_hqU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ds')"
      ],
      "metadata": {
        "id": "2fx6-PYy3-IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-xlXfslhi5t"
      },
      "outputs": [],
      "source": [
        "Tensor = list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "g0Bxm1xuiQVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shape(tensor: Tensor) -> List[int]:\n",
        "  sizes: List[int] = []\n",
        "  while isinstance(tensor, list):\n",
        "    sizes.append(len(tensor))\n",
        "    tensor = tensor[0]\n",
        "  return sizes"
      ],
      "metadata": {
        "id": "_6skEWrgiUhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert shape([1,2,3])"
      ],
      "metadata": {
        "id": "CVqahEBnizGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_1d(tensor: Tensor) -> bool:\n",
        "  return not isinstance(tensor[0], list)"
      ],
      "metadata": {
        "id": "MUhrL4r3i3_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_sum(tensor: Tensor) -> float:\n",
        "  if is_1d(tensor):\n",
        "    return sum(tensor)\n",
        "  else:\n",
        "    return sum(tensor_sum(tensor_i) for tensor_i in tensor)"
      ],
      "metadata": {
        "id": "9BlEUar8jIll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable"
      ],
      "metadata": {
        "id": "hp6F76VRjj4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_apply(f: Callable[[float], float], tensor: Tensor) -> Tensor:\n",
        "  if is_1d(tensor):\n",
        "    return [f(x) for x in tensor]\n",
        "  else:\n",
        "    return [tensor_apply(f, tensor_i) for tnesor_i in tensor]"
      ],
      "metadata": {
        "id": "2ghtTYG1jrvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fun_ex(x: float) -> float:\n",
        "  return 2*x-3"
      ],
      "metadata": {
        "id": "1MiUA3feqwvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zeros_like(tensor: Tensor) -> Tensor:\n",
        "  return tensor_apply(lambda _: 0.0, tensor)"
      ],
      "metadata": {
        "id": "-u_Dyv59lJRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_combine(f: Callable[[float, float], float],\n",
        "                   t1: Tensor,\n",
        "                   t2: Tensor) -> Tensor:\n",
        "    if is_1d(t1):\n",
        "        return [f(x, y) for x, y in zip(t1, t2)]\n",
        "    else:\n",
        "        return [tensor_combine(f, t1_i, t2_i)\n",
        "                for t1_i, t2_i in zip(t1, t2)]"
      ],
      "metadata": {
        "id": "5lSVxIV1rdAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 층 추상화"
      ],
      "metadata": {
        "id": "oLmYyMZOmJo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Tuple"
      ],
      "metadata": {
        "id": "ml_gR6tDlqQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "  def forward(self, input):\n",
        "    raise NotImplementedError    \n",
        "  def backward(self, gradient):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "F-OwItyomaeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params(self)->Iterable[Tensor]:\n",
        "    return()\n",
        "\n",
        "def grads(self)->Iterable[Tensor]:\n",
        "    return()"
      ],
      "metadata": {
        "id": "DKt4hdkeZtBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_networks import sigmoid"
      ],
      "metadata": {
        "id": "jsXgpNWHnJP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Layer):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Apply sigmoid to each element of the input tensor,\n",
        "        and save the results to use in backpropagation.\n",
        "        \"\"\"\n",
        "        self.sigmoids = tensor_apply(sigmoid, input)\n",
        "        return self.sigmoids\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        return tensor_combine(lambda sig, grad: sig * (1 - sig) * grad,\n",
        "                              self.sigmoids,\n",
        "                              gradient)"
      ],
      "metadata": {
        "id": "W3NA_A7i8k6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 선형 층"
      ],
      "metadata": {
        "id": "hWfs7Iuo9FLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "from probability import inverse_normal_cdf\n",
        "\n",
        "def random_uniform(*dims: int) -> Tensor:\n",
        "    if len(dims) == 1:\n",
        "        return [random.random() for _ in range(dims[0])]\n",
        "    else:\n",
        "        return [random_uniform(*dims[1:]) for _ in range(dims[0])]"
      ],
      "metadata": {
        "id": "uegTHWwm84fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_normal(*dims: int,\n",
        "                  mean: float = 0.0,\n",
        "                  variance: float = 1.0) -> Tensor:\n",
        "    if len(dims) == 1:\n",
        "        return [mean + variance * inverse_normal_cdf(random.random())\n",
        "                for _ in range(dims[0])]\n",
        "    else:\n",
        "        return [random_normal(*dims[1:], mean=mean, variance=variance)\n",
        "                for _ in range(dims[0])]"
      ],
      "metadata": {
        "id": "xUe0Sffh882J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_tensor(*dims: int, init: str = 'normal') -> Tensor:\n",
        "    if init == 'normal':\n",
        "        return random_normal(*dims)\n",
        "    elif init == 'uniform':\n",
        "        return random_uniform(*dims)\n",
        "    elif init == 'xavier':\n",
        "        variance = len(dims) / sum(dims)\n",
        "        return random_normal(*dims, variance=variance)\n",
        "    else:\n",
        "        raise ValueError(f\"unknown init: {init}\")"
      ],
      "metadata": {
        "id": "RErjbYdt8_Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from linear_algebra import dot\n",
        "\n",
        "class Linear(Layer):\n",
        "    def __init__(self, input_dim: int, output_dim: int, init: str = 'xavier') -> None:\n",
        "    \n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.w = random_tensor(output_dim, input_dim, init=init)\n",
        "\n",
        "        self.b = random_tensor(output_dim, init=init)"
      ],
      "metadata": {
        "id": "ibznmLQ29BVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input: Tensor) -> Tensor:\n",
        "        self.input = input\n",
        "        return [dot(input, self.w[o]) + self.b[o]\n",
        "                for o in range(self.output_dim)]\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "XZuKMoGO9TPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(self, gradient: Tensor) -> Tensor:\n",
        "\n",
        "        self.b_grad = gradient\n",
        "\n",
        "        self.w_grad = [[self.input[i] * gradient[o]\n",
        "                        for i in range(self.input_dim)]\n",
        "                       for o in range(self.output_dim)]\n",
        "\n",
        "        return [sum(self.w[o][i] * gradient[o] for o in range(self.output_dim))\n",
        "                for i in range(self.input_dim)]"
      ],
      "metadata": {
        "id": "ggKk7UuI9bmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def params(self) -> Iterable[Tensor]:\n",
        "        return [self.w, self.b]\n",
        "\n",
        "def grads(self) -> Iterable[Tensor]:\n",
        "        return [self.w_grad, self.b_grad]\n"
      ],
      "metadata": {
        "id": "Q5w_jlCC9pZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순차적 층으로 구성된 신경망"
      ],
      "metadata": {
        "id": "e8F9_e9h9yC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "l_pOZcde9wcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sequential(Layer):\n",
        "  \n",
        "    def __init__(self, layers: List[Layer]) -> None:\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, gradient):\n",
        "        for layer in reversed(self.layers):\n",
        "            gradient = layer.backward(gradient)\n",
        "        return gradient\n",
        "\n",
        "    def params(self) -> Iterable[Tensor]:\n",
        "        return (param for layer in self.layers for param in layer.params())\n",
        "\n",
        "    def grads(self) -> Iterable[Tensor]:\n",
        "        return (grad for layer in self.layers for grad in layer.grads())"
      ],
      "metadata": {
        "id": "fSkdWuu_-F-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실함수와 최적화"
      ],
      "metadata": {
        "id": "XEFmKRd6-dFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss:\n",
        "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "GAMEdWrp-cih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSE(Loss):\n",
        "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "        squared_errors = tensor_combine(\n",
        "            lambda predicted, actual: (predicted - actual) ** 2,\n",
        "            predicted,\n",
        "            actual)\n",
        "\n",
        "        return tensor_sum(squared_errors)\n",
        "    \n",
        "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "        return tensor_combine(\n",
        "            lambda predicted, actual: 2 * (predicted - actual),\n",
        "            predicted,\n",
        "            actual)"
      ],
      "metadata": {
        "id": "_7bAm7nk-la5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer:\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "o0DgCSjLATtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientDescent(Optimizer):\n",
        "    def __init__(self, learning_rate: float = 0.1) -> None:\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        for param, grad in zip(layer.params(), layer.grads()):\n",
        "            # Update param using a gradient step\n",
        "            param[:] = tensor_combine(\n",
        "                lambda param, grad: param - grad * self.lr,\n",
        "                param,\n",
        "                grad)"
      ],
      "metadata": {
        "id": "2kd23Q-8AYpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Momentum(Optimizer):\n",
        "    def __init__(self, learning_rate: float, momentum: float = 0.9) -> None:\n",
        "        self.lr = learning_rate\n",
        "        self.mo = momentum\n",
        "        self.updates: List[Tensor] = []  # running average\n",
        "\n",
        "    def step(self, layer: Layer) -> None:\n",
        "        if not self.updates:\n",
        "            self.updates = [zeros_like(grad) for grad in layer.grads()]\n",
        "\n",
        "        for update, param, grad in zip(self.updates, layer.params(), layer.grads()):\n",
        "           \n",
        "            update[:] = tensor_combine(lambda u, g: self.mo * u + (1 - self.mo) * g, update, grad)\n",
        "\n",
        "            param[:] = tensor_combine(lambda p, u: p - self.lr * u, param, update)"
      ],
      "metadata": {
        "id": "Ayp5G2C3AinR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XOR 문제 다시 풀기"
      ],
      "metadata": {
        "id": "yuIH4VMhBdl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [[0., 0], [0., 1], [1., 0], [1., 1]]\n",
        "ys = [[0.], [1.], [1.], [0.]]"
      ],
      "metadata": {
        "id": "EO3LzpHtB6W2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)"
      ],
      "metadata": {
        "id": "Z22QIAsrCYX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Sequential([\n",
        "    Linear(input_dim = 2, output_dim = 2),\n",
        "    Sigmoid(),\n",
        "    Linear(input_dim = 2, output_dim = 1)              \n",
        "])"
      ],
      "metadata": {
        "id": "s8b367oKCbwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "pFs6CjtjC5XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = GradientDescent(learning_rate = 0.1)\n",
        "loss = SSE()\n",
        "\n",
        "with tqdm.trange(3000) as t:\n",
        "  for epoch in t:\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "      predicted = net.forward(x)\n",
        "      epoch_loss += loss.loss(predicted, y)\n",
        "      gradient = loss.gradient(predicted, y)\n",
        "      net.backward(gradient)\n",
        "\n",
        "      optimizer.step(net)\n",
        "    \n",
        "    t.set_description(f\"xor loss {epoch_loss:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "nh3tTiETC8FD",
        "outputId": "8eca3d98-5660-4355-810f-c5b2432750c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-42b9c578ee2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-e1c9e6a4c948>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-7e9f4547c2cd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vF6T1QbfDFcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#다른 활성화 함수"
      ],
      "metadata": {
        "id": "ak1PMfR_At0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "xYfabj8nApTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x: float) -> float:\n",
        "    if x < -100:  return -1\n",
        "    elif x > 100: return 1\n",
        "\n",
        "    em2x = math.exp(-2 * x)\n",
        "    return (1 - em2x) / (1 + em2x)"
      ],
      "metadata": {
        "id": "vAChYuAAEhJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tanh(Layer):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        self.tanh = tensor_apply(tanh, input)\n",
        "        return self.tanh\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        return tensor_combine(\n",
        "            lambda tanh, grad: (1 - tanh ** 2) * grad,\n",
        "            self.tanh,\n",
        "            gradient)"
      ],
      "metadata": {
        "id": "ZQ-YsN6mEotf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu(Layer):\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        self.input = input\n",
        "        return tensor_apply(lambda x: max(x, 0), input)\n",
        "\n",
        "    def backward(self, gradient: Tensor) -> Tensor:\n",
        "        return tensor_combine(lambda x, grad: grad if x > 0 else 0, self.input, gradient)"
      ],
      "metadata": {
        "id": "ji2O3P_CE01x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fizz Buzz"
      ],
      "metadata": {
        "id": "xazeTuF0E7-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from neural_networks import binary_encode, fiz_buzz_encode, argmax"
      ],
      "metadata": {
        "id": "jgxVsDULE6Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs = [binary_encode(n) for n in range(101, 1024)]\n",
        "ys = [fiz_buzz_encode(n) for n in range(101, 1024)]"
      ],
      "metadata": {
        "id": "RKf75XMiGdWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_HIDDEN = 25"
      ],
      "metadata": {
        "id": "S7aT3-8MGtz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)"
      ],
      "metadata": {
        "id": "JlMxg6jlGwz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Sequential([  \n",
        "    Linear(input_dim = 10, output_dim = NUM_HIDDEN, init = 'uniform'),\n",
        "    Tanh(),\n",
        "    Linear(input_dim = NUM_HIDDEN, output_dim = 4, init = 'uniform'),\n",
        "    Sigmoid(), \n",
        "    ])"
      ],
      "metadata": {
        "id": "KDnCZpdFG0Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fizzbuzz_accuracy(low: int, hi: int, net: Layer) -> float:\n",
        "  num_correct = 0\n",
        "  for n in range(low, hi):\n",
        "    x = binary_encode(n)\n",
        "    predicted = argmax(net.forward(x))\n",
        "    actual = argmax(fizz_buzz_encode)\n",
        "    if predicted == actual:\n",
        "      num_correct += 1\n",
        "  \n",
        "  return num_correct / (hi - low)"
      ],
      "metadata": {
        "id": "UxTvt70bHac4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Momentum(learning_rate = 0.1, momentum = 0.9)\n",
        "loss = SSE()"
      ],
      "metadata": {
        "id": "NPyjv16pIlry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tqdm.trange(3000) as t:\n",
        "  for epoch in t:\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for x, y in zip(xs, ys):\n",
        "      predicted = net.forward(x)\n",
        "      epoch_loss += loss.loss(predicted, y)\n",
        "      gradient = loss.gredient(predicted, y)\n",
        "      net.backward(gredient)\n",
        "\n",
        "      optimizer.step(net)\n",
        "    \n",
        "    accuracy = fizzbuzz_accuracy(101, 1024, net)\n",
        "    t.set_description(f\"fb loss: {epoch_loss: .2f} acc: {accuracy: .2f}\")"
      ],
      "metadata": {
        "id": "bSmQEYuvIusY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test result\", fizzbuzz_accuracy(1, 101, net))"
      ],
      "metadata": {
        "id": "GB4s9l6_Kbd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SoftMax와 Cross_Entry"
      ],
      "metadata": {
        "id": "Wut9bIc_E-sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(tensor: Tensor) -> Tensor:\n",
        "    \"\"\"Softmax along the last dimension\"\"\"\n",
        "    if is_1d(tensor):\n",
        "        largest = max(tensor)\n",
        "        exps = [math.exp(x - largest) for x in tensor]\n",
        "\n",
        "        sum_of_exps = sum(exps)                 \n",
        "        return [exp_i / sum_of_exps for exp_i in exps]              \n",
        "    else:\n",
        "        return [softmax(tensor_i) for tensor_i in tensor]"
      ],
      "metadata": {
        "id": "Y9JmFFiEGcBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCrossEntropy(Loss):\n",
        "    def loss(self, predicted: Tensor, actual: Tensor) -> float:\n",
        "        probabilities = softmax(predicted)\n",
        "\n",
        "        likelihoods = tensor_combine(lambda p, act: math.log(p + 1e-30) * act,\n",
        "                                     probabilities,\n",
        "                                     actual)\n",
        "\n",
        "        return -tensor_sum(likelihoods)\n",
        "\n",
        "    def gradient(self, predicted: Tensor, actual: Tensor) -> Tensor:\n",
        "        probabilities = softmax(predicted)\n",
        "\n",
        "        return tensor_combine(lambda p, actual: p - actual,\n",
        "                              probabilities,\n",
        "                              actual)"
      ],
      "metadata": {
        "id": "gG_14dhHKvtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 드롭아웃"
      ],
      "metadata": {
        "id": "Mm7GWRQFK-wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l_r2Xz-LK9A7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}